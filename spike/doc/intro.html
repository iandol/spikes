<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head>	<title>Spike Train Analysis Toolkit Documentation</title>	<link rel="stylesheet" type="text/css" href="style.css" title="default" /></head><body>	<table>		<tr>			<td><a href="index.html"><img src="images/logo2.png" /></a></td>			<td>				<h1>Spike Train Analysis Toolkit</h1>			</td>		</tr>	</table>	<p>Information theoretic methods are now widely used for the analysis of spike train data. However, developing robust implementations of these methods can be tedious and time-consuming. In order to facilitate further adoption of these methods, we have developed the Spike Train Analysis Toolkit, a software package which implements several information-theoretic spike train analysis techniques. This implementation behaves like a typical Matlab toolbox, but the underlying computations are coded in C and optimized for efficiency.</p>	<p>In describing the capabilities of the toolkit, we further distinguish between what we call <em>information methods</em> and <em>entropy methods</em>.</p>	<h2>Getting started</h2>	<p>		How can I tell how much information is conveyed by the neural responses I have recorded?		<ul>			<li>My experiments involve recording neural responses in two or more behavioral contexts, or in response to two or more stimuli. To what extent do my neural data represent, or encode, these categories?</li>			<em>Use the direct categorical information method, metric space method, or binless method.</em>			<li>My experiments record repeated responses to the same continuously changing stimulus. How can I measure the amount of information that the neural responses contain?</li>			<em>Use the direct formal information method or the context tree method.</em>		</ul>	</p>	<p>To help you get started, we have created <a href="demo.html">demonstrations</a> of the various information methods, which you should use as a template for your own analyses. The demonstrations illustrate how to select and use these methods, and what to look for in the output. They also guide the selection and use of the associated entropy methods, options, and parameters.</p>	<p>The direct method makes the fewest assumptions about the nature of the neural code, but generally requires the most data (e.g., hundreds of repeats for some stimuli). The metric space method and the binless method require only about 10 repeats per stimulus, but make assumptions about the nature of the neural code (see the references below). For multineuronal data, only the direct and metric space methods are applicable. The direct methods require one to consider time in discrete bins (i.e., data are symbol sequences), whereas the metric space and binless methods work with continuous time (i.e., data are point processes).</p>	<p>This is enough to get started; further detail on the various methods contained in the STAToolkit is presented below.</p>	<h2>Information methods</h2>	<p>Information methods are those methods which estimate the mutual information between an ensemble of spike trains and some other experimental variable. We distinguish between <em>formal</em> and <em>attribute-specific</em> information, as proposed by <a href="http://jn.physiology.org/cgi/content/abstract/85/1/305">Reich et al. (2001)</a>. Formal information concerns all aspects of the response that depend on the stimulus. It is estimated from the difference between the entropy of responses to an ensemble of temporally rich stimuli and the entropy of responses to an ensemble of repeated stimuli. Attribute-specific information refers to the amount of information that responses convey about a particular experimental parameter. If the parameter describes one of several discrete categories, we refer to it as <em>category-specific information</em>.</p>	<p>The current version contains implementations of four information methods:</p>	<dl>		<dt>Direct method (formal and category-specific information)</dt>		<dd>Strong, S.P., Koberle, R., de Ruyter van Steveninck, R.R. and Bialek, W. (1998). <a href="http://dx.doi.org/10.1103/PhysRevLett.80.197">Entropy and Information in Neural Spike Trains.</a> <i>Physical Review Letters</i>. 80: 197-200.</dd>		<dt>Metric space method (category-specific information)</dt>		<dd>Victor, J.D., and Purpura, K.P. (1997). <a href="http://dx.doi.org/10.1088/0954-898X/8/2/003">Metric-space analysis of spike trains: theory, algorithms and application</a>. <i>Network: Computation in Neural Systems</i>. 8: 127-164.<br />		Aronov, D. (2003). <a href="http://dx.doi.org/10.1016/S0165-0270(03)00006-2">Fast algorithm for the metric-space analysis of simultaneous responses of multiple single neurons</a>. <i>Journal of Neuroscience Methods</i>, 124: 175-179.</dd>		<dt>Binless method (category-specific information)</dt>		<dd>Victor, J.D. (2002). <a href="http://dx.doi.org/10.1103/PhysRevE.66.051903">Binless strategies for estimation of information from neural data</a>. <i>Physical Review E</i>. 66: 051903</dd>		<dt>Context tree method (formal information)</dt>		<dd>Kennel, M., Shlens, J., Abarbanel, H., and Chichilnisky, E.J. (2005) <a href="http://dx.doi.org/doi:10.1162/0899766053723050">Estimating entropy rates with Bayesian confidence intervals</a>. <i>Neural Computation</i>, 2005: 17, 1531-1576.</dd>		<dd>Shlens, J., Kennel, M., Abarbanel, H., and Chichilnisky, E.J. (2007) <a href="http://dx.doi.org/doi:10.1162/neco.2007.19.7.1683">Estimating information rates in neural spike trains with confidence intervals</a>. <i>Neural Computation</i>, 2007: 19, 1683-1719.</dd>	</dl>The implementations of the direct method and the metric space have extensions for the analysis of simultaneously recorded spike trains.        <h2>Entropy methods</h2>	<p>Entropy methods are those methods that estimate entropy from a discrete histogram, a computation common to many information-theoretic methods. For the general user, we recommend adapting the demos for one's own data; the demos select appropriate entropy methods. The advanced user may wish to substitute other entropy methods, or to use the entropy methods as standalone modules (e.g., <a href="function/entropy1d.html">entropy1d</a>). Entropy methods are chosen by including the appropriate code in the <code>entropy_estimation_method</code> option (see <a href="opts_info.html">information options and parameters</a> and implied <a href="opts_entropy.html">entropy options and parameters</a>). The included methods are:</p>	<dl>		<dt>Plug-in (<code>plugin</code>)</dt>		<dd>This is the classical estimator, based on the entropy formula <span class="math"><i>H</i> = &Sigma;<sub><i>i</i></sub> <i>p</i><sub><i>i</i></sub> log<sub>2</sub><i>p</i><sub><i>i</i></sub></span>.</dd>		<dt>Treves-Panzeri-Miller-Carlton (<code>tpmc</code>)</dt>		<dd>Treves, A. and Panzeri, S. (1995). The Upward Bias in Measures of Information Derived from Limited Data Samples. <i>Neural Computation</i>, 7: 399-407.</dd>		<dd>Miller, G.A. (1955). Note on the bias on information estimates. <i>Information Theory in Psychology, Problems and Methods</i>. II-B: 95-100.</dd>		<dd>Carlton, A.G. (1969). On the bias of information estimates. <i>Psychological Bulletin</i>. 71: 108-109.</dd>		<dt>Jackknife (<code>jack</code>)</dt>		<dd>Efron, B. and Tibshirani, R.J. (1993). <i>An introduction to the bootstrap</i>. Chapman &amp; Hall.</dd>		<dt>Ma bound (<code>ma</code>)</dt>		<dd>Ma, S. (1981). <a href="http://dx.doi.org/10.1007/BF01013169">Calculation of Entropy from Data of Motion</a>. <i>Journal of Statistical Physics</i>. 26: 221-240.<br />		The version included in the toolkit is debiased, as presented in Strong et al. (1998).</dd>		<dt>Best upper bound (<code>bub</code>)</dt>		<dd>Paninski, L. (2003). <a href="http://www.mitpressjournals.org/doi/abs/10.1162/089976603321780272">Estimation of Entropy and Mutual Information</a>. <i>Neural Computation</i>. 15: 1191-1253.</dd>		<dt>Chao-Shen (<code>chaoshen</code>)</dt>		<dd>Chao, A. and Shen, T.-J. (2003). <a href="http://dx.doi.org/10.1023/A:1026096204727">Nonparametric estimation of Shannon's index of diversity when there are unseen species in a sample</a>. <i>Environmental and Ecological Statistics</i>. 10: 429-443.</dd>		<dt>Wolpert-Wolf&mdash;Bayesian with a Dirichlet prior (<code>ww</code>)</dt>		<dd>Wolpert, D.H. and Wolf, D.R. (1995). <a href="http://dx.doi.org/10.1103/PhysRevE.52.6841">Estimating functions of probability from a finite set of samples</a>. <i>Physical Review E</i>. 52: 6841-6854.<br />		<a href="http://dx.doi.org/10.1103/PhysRevE.54.6973.2">Erratum</a> in <i>Physical Review E</i>. 54: 6973.</dd>		<dd>Wolpert, D.H. and Wolf, D.R. (1994). <a href="http://arxiv.org/abs/comp-gas/9403001">Estimating Functions of Probability Distributions from a Finite Set of Samples, Part 1: Bayes Estimators and the Shannon Entropy</a>. ar&chi;iv comp-gas/9403001.</dd>		<dd>Wolpert, D.H. and Wolf, D.R. (1994). <a href="http://arxiv.org/abs/comp-gas/9403002">Estimating Functions of Distributions from A Finite Set of Samples, Part 2: Bayes Estimators for Mutual Information, Chi-Squared, Covariance and other Statistics</a>. ar&chi;iv comp-gas/9403002.</dd>		<dt>Nemenman-Shafee-Bialek (<code>nsb</code>)</dt>		<dd>Nemenman, I., Shafee, F., Bialek, W. (2002) <a href="">Entropy and inference, revisited.</a> In Dietterich, T.G., Becker, S., and Ghahramani, Z. eds. Advances in Neural Information Processing Systems 14, Cambridge, MA, 2002. MIT Press. arXiv: physics/0108025.</dd>		<dd>See also the original authors' <a href="http://nsb-entropy.sourceforge.net/">NSB Entropy Estimation</a> project page at SourceForge.</dd>	</dl>	<h3>Methods for estimating the variance of entropy estimates</h3>	The toolkit also provides estimates of the variance of entropy estimates, which can in turn be used to compute confidence limits. These results can be obtained by setting the option <code>variance_estimation_method</code> (see <a href="opts_info.html">information options</a>). The jackknife (<code>jack</code>) and bootstrap (<code>boot</code>) methods can be applied to any entropy estimate. Additionally, the toolkit includes a variance estimate that is specific to the NSB entropy method (<code>nsb_var</code>), and may include other specific variance estimates in the future.	<h2>Modular implementation of information methods</h2>	<p>Each information method has a top level function which performs an analysis on an input data stucture. Each information method has been partitioned into modules corresponding to steps that provide useful intermediate results. They also include top-level functions for users that do not require flexibility. The table below depicts the five major information methods and the functions they call. Function <code>directformal</code> performs a formal information analysis via the direct method; function <code>directcat</code> performs a categorical information analysis via the direct method; function <code>metric</code> performs a categorical information analysis via the metric space method; function <code>binless</code> performs a categorical information analysis via the binless method; function <code>ctwmcmc</code> performs a formal information analysis via the context-tree method.</p>	<table border="1">		<tr valign="top">			<th width="25%"><code><a href="function/directformal.html" target="_blank">directformal</a></code></th>			<th width="25%"><code><a href="function/directcat.html" target="_blank">directcat</a></code></th>			<th width="25%"><code><a href="function/metric.html" target="_blank">metric</a></code></th>			<th width="25%"><code><a href="function/binless.html" target="_blank">binless</a></code></th>			<th width="25%"><code><a href="function/ctwmcmc.html" target="_blank">ctwmcmc</a></code></th>		</tr>		<tr valign="top">			<td><code><a href="function/directbin.html" target="_blank">directbin</a></code> Bin spike trains</td>			<td><code><a href="function/directbin.html" target="_blank">directbin</a></code> Bin spike trains</td>			<td><code><a href="function/metricopen.html" target="_blank">metricopen</a></code> Prepare input data structure</td>			<td><code><a href="function/binlessopen.html" target="_blank">binlessopen</a></code> Prepare input data structure</td>			<td><code><a href="function/directbin.html" target="_blank">directbin</a></code> Bin spike trains</td>		</tr>		<tr valign="top">			<td><code><a href="function/directcondformal.html" target="_blank">directcondformal</a></code> Condition data on both category and time slice</td>			<td>&nbsp;</td>			<td><code><a href="function/metricdist.html" target="_blank">metricdist</a></code> Compute distances between sets of spike train pairs.</td>			<td><code><a href="function/binlesswarp.html" target="_blank">binlesswarp</a></code> Warp spike times</td>			<td><code><a href="function/ctwmcmctree.html" target="_blank">ctwmcmctree</a></code> Build the context tree(s) from data.</td>		</tr>		<tr valign="top">			<td><code><a href="function/directcounttotal.html" target="_blank">directcounttotal</a></code> Count spike train words disregarding class</td>			<td><code><a href="function/directcountcond.html" target="_blank">directcountcond</a></code> Count spike train words in each class and disregarding class</td>			<td><code><a href="function/metricclust.html" target="_blank">metricclust</a></code> Cluster spike trains based on distance matrix.</td>			<td><code><a href="function/binlessembed.html" target="_blank">binlessembed</a></code> Embed the spike trains</td>			<td><code><a href="function/ctwmcmcsample.html" target="_blank">ctwmcmcsample</a></code> Perform Markov chain Monte Carlo sampling on context tree(s).</td>		</tr>		<tr valign="top">			<td>&nbsp;</td>			<td>&nbsp;</td>			<td><code><a href="function/matrix2hist2d.html" target="_blank">matrix2hist2d</a></code> Converts a 2-D matrix of counts to a 2-D histogram</td>			<td>&nbsp;</td>			<td>&nbsp;</td>		</tr>		<tr valign="top">			<td><code><a href="function/infocond.html" target="_blank">infocond</a></code> Information and entropies from conditional and total histograms</td>			<td><code><a href="function/infocond.html" target="_blank">infocond</a></code> Information and entropies from conditional and total histograms</td>			<td><code><a href="function/info2d.html" target="_blank">info2d</a></code> Information and entropies from a 2-D histogram</td>			<td><code><a href="function/binlessinfo.html" target="_blank">binlessinfo</a></code> Compute information components</td>			<td><code><a href="function/ctwmcmcinfo.html" target="_blank">ctwmcmcinfo</a></code> Compute information from context tree entropies.</td>		</tr>	</table>	<p>All of the functions are documented in the <a href="function/index.html" target="_blank">function reference</a> (note: opens in a new browser window).</p>	<p>Included <a href="demo.html">demos</a> give examples of how the top-level functions can be used.</p>	<h2>Inputs to the toolkit</h2>	<p>We have developed a <a href="format.html">text-based input file format</a> for the toolkit that is easy to generate. Users also have the option of bypassing the text-based file format and using another means to read the data into the <a href="input_struct.html">Matlab input data structure</a>.</p>	<p>Documentation fo the analysis options and parameters for <a href="opts_info.html">information methods</a> and <a href="opts_entropy.html">entropy methods</a> is available.</p>	<h2>Outputs from the toolkit</h2>	<p>Estimated quantities are packaged in data structures with auxillary information such as variance estimates. See <a href="output_struct.html">this page</a> for more information.</p>	<h2>The future</h2>	<p>This toolkit is one component of a larger endeavor in the field of computational neuroinformatics. We are in the process of integrating the toolkit with a <a href="http://neurodatabase.org">Neurodatabase.org</a> (a publicly-accessable neurophysiology database), developing a web-based analysis interface, and adapting the toolkit for a dedicated parallel cluster.</p>	<p>We are also working with members of the computational neuroscience community to incorporate their information theoretic techniques, as well as looking beyond information theory to other methodologies for analyzing neuroscience data. Please contact us if you would like to contribute.</p></body></html>